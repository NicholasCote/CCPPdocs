{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the CISL Cloud Pilot Project (CCPP) Follow our up to date Kanban . Pilot This is currently a Pilot project. Determining long term feasibility is one of our objectives. Vision Provide and operate an on-premise cloud offering for the scientific community to supplement traditional HPC services and public cloud offerings while utilizing 2i2c to host a JupyterHub instance in the public cloud. What is an on-premise cloud? NCAR | CISL runs Compute, Storage & Network hardware in robust Data Centers at multiple organizational facilities. An on-premise cloud is offering users the ability to utilize those highly available organizationally supported compute resources for approved use cases. This includes access to routable network space and UCAR Domain Name Systems (DNS). Security standards set by the organization are implemented and controlled by administrators to make sure internal policies are being adhered to. These resources would be provided to supplement computing needs that aren't fulfilled by the HPC offering, public cloud, or what is available to you locally. Goals Improve understanding of how scientific community might use and benefit from an on-prem cloud Which services fit on-prem better than traditional HPC and/or public cloud Gain experience within CISL deploying and operating an on-prem cloud and associated services Improve CISL ability to support interactive analysis workflows in environment where data is globally distributed Increase user visibility in to on-prem cloud offerings Develop metrics to showcase project value & feasibility Gain experience with Agile Project Management","title":"Home"},{"location":"#welcome-to-the-cisl-cloud-pilot-project-ccpp","text":"Follow our up to date Kanban .","title":"Welcome to the CISL Cloud Pilot Project (CCPP)"},{"location":"#pilot","text":"This is currently a Pilot project. Determining long term feasibility is one of our objectives.","title":"Pilot"},{"location":"#vision","text":"Provide and operate an on-premise cloud offering for the scientific community to supplement traditional HPC services and public cloud offerings while utilizing 2i2c to host a JupyterHub instance in the public cloud.","title":"Vision"},{"location":"#what-is-an-on-premise-cloud","text":"NCAR | CISL runs Compute, Storage & Network hardware in robust Data Centers at multiple organizational facilities. An on-premise cloud is offering users the ability to utilize those highly available organizationally supported compute resources for approved use cases. This includes access to routable network space and UCAR Domain Name Systems (DNS). Security standards set by the organization are implemented and controlled by administrators to make sure internal policies are being adhered to. These resources would be provided to supplement computing needs that aren't fulfilled by the HPC offering, public cloud, or what is available to you locally.","title":"What is an on-premise cloud?"},{"location":"#goals","text":"Improve understanding of how scientific community might use and benefit from an on-prem cloud Which services fit on-prem better than traditional HPC and/or public cloud Gain experience within CISL deploying and operating an on-prem cloud and associated services Improve CISL ability to support interactive analysis workflows in environment where data is globally distributed Increase user visibility in to on-prem cloud offerings Develop metrics to showcase project value & feasibility Gain experience with Agile Project Management","title":"Goals"},{"location":"how-to/agile/","text":"Agile Interaction The CCPP team is utilizing hybrid Agile Project Management strategies (Kanban & Waterfall) to manage development. What is Agile Project Management? At a high level Agile is a framework that focuses on interaction, collaboration, and visibility in order to predictably deliver working product that meets all customer requirements. What is Waterfall? Waterfall methodology is a project management approach that maps out a project into distinct phases. The CCPP team will be utilizing Waterfall to manage overall project status and track deliverables. The current Waterfall smartsheet rollup can be viewed here . What is Kanban? Kanban is a flexible Agile framework with a focus on continuous delivery. Individual work items are called User Stories and are represented visually on a Kanban Board. What are User Stories? An informal explanation of a feature written from the perspective of the end user. The goal is to keep this short and simple so that it can be accomplished in a few days. If it will take more than a few days the issue needs to be looked at in more detail and broken down in to smaller batch sizes. This allows for a continuous feedback loop where small pieces of overall functionality are demonstrated to the user base. These small pieces of functionality are defined in the User Stories Acceptance Criteria. What is Acceptance Criteria? In order to make sure the User Story has a clear definition of being done the PO will define Acceptance Criteria. It is written from a user perspective and defines a requirement, why it is needed, and what expectations are for successful completion. Example: As a CCPP User I want to understand what Acceptance Criteria is so that I can ensure my completed user story meets my expectations of done. How do I interact with the team? Via the Agile Product Owner (PO) All requests & questions can be directed to the Product Owner (PO) for the project. Nick Cote is the Agile Product owner for this project. The Agile PO's responsibility is representing the customer and stakeholder and is their interface to the development team. This allows the development team members to focus on delivering valuable working product while the customer is still being accurately represented via the PO. All requests directed at the PO will be turned in to user stories on our Kanban Board. Kanban Board Issues can also be created in Jira and added to our Kanban backlog. The PO will ensure your request is understood and has valid acceptance criteria. It will then be prioritized appropriately against other tasks in the queue. The CCPP Kanban board currently implements the follow story states: Backlog # New stories that have not yet been fully reviewed and prioritized To Do # Stories have been reviewed and prioritized Stalled # Work is on hold for various reasons In Progress # A team member is actively working the Story In Review # PO will check to make sure the Story Acceptance Criteria is met Done # Acceptance Criteria has been verified successfully and the Story is complete User Stories are self-assigned by a team member who wants to accomplish the task. A User Story will move from a status of To Do \u2192 In Progress when it is the at the top of the Assignee's To Do list and they are ready to start. Kanban utilizes Work In Progress (WIP) limits to help identify bottlenecks and promoting moving tasks to Done . The CCPP has implemented the following WIP limits: Stalled : 4 In Progress : 6 In Review : 4 Demonstrations As new functionality is rolled out we will be taking advantage of different platforms to demonstrate working products to our stakeholders. Feedback is strongly desired during these demonstrations in order to ensure what the team is delivering is accurately meeting the user requirements. These interactions make sure the team is continuously improving. Retrospectives Once a month the team will look back on the process overall. The focus will be on what has been working, what hasn't been working, and what improvements should we try in order to improve the process for our workflow.","title":"Agile Interaction"},{"location":"how-to/agile/#agile-interaction","text":"The CCPP team is utilizing hybrid Agile Project Management strategies (Kanban & Waterfall) to manage development.","title":"Agile Interaction"},{"location":"how-to/agile/#what-is-agile-project-management","text":"At a high level Agile is a framework that focuses on interaction, collaboration, and visibility in order to predictably deliver working product that meets all customer requirements.","title":"What is Agile Project Management?"},{"location":"how-to/agile/#what-is-waterfall","text":"Waterfall methodology is a project management approach that maps out a project into distinct phases. The CCPP team will be utilizing Waterfall to manage overall project status and track deliverables. The current Waterfall smartsheet rollup can be viewed here .","title":"What is Waterfall?"},{"location":"how-to/agile/#what-is-kanban","text":"Kanban is a flexible Agile framework with a focus on continuous delivery. Individual work items are called User Stories and are represented visually on a Kanban Board.","title":"What is Kanban?"},{"location":"how-to/agile/#what-are-user-stories","text":"An informal explanation of a feature written from the perspective of the end user. The goal is to keep this short and simple so that it can be accomplished in a few days. If it will take more than a few days the issue needs to be looked at in more detail and broken down in to smaller batch sizes. This allows for a continuous feedback loop where small pieces of overall functionality are demonstrated to the user base. These small pieces of functionality are defined in the User Stories Acceptance Criteria.","title":"What are User Stories?"},{"location":"how-to/agile/#what-is-acceptance-criteria","text":"In order to make sure the User Story has a clear definition of being done the PO will define Acceptance Criteria. It is written from a user perspective and defines a requirement, why it is needed, and what expectations are for successful completion. Example: As a CCPP User I want to understand what Acceptance Criteria is so that I can ensure my completed user story meets my expectations of done.","title":"What is Acceptance Criteria?"},{"location":"how-to/agile/#how-do-i-interact-with-the-team","text":"","title":"How do I interact with the team?"},{"location":"how-to/agile/#via-the-agile-product-owner-po","text":"All requests & questions can be directed to the Product Owner (PO) for the project. Nick Cote is the Agile Product owner for this project. The Agile PO's responsibility is representing the customer and stakeholder and is their interface to the development team. This allows the development team members to focus on delivering valuable working product while the customer is still being accurately represented via the PO. All requests directed at the PO will be turned in to user stories on our Kanban Board.","title":"Via the Agile Product Owner (PO)"},{"location":"how-to/agile/#kanban-board","text":"Issues can also be created in Jira and added to our Kanban backlog. The PO will ensure your request is understood and has valid acceptance criteria. It will then be prioritized appropriately against other tasks in the queue. The CCPP Kanban board currently implements the follow story states: Backlog # New stories that have not yet been fully reviewed and prioritized To Do # Stories have been reviewed and prioritized Stalled # Work is on hold for various reasons In Progress # A team member is actively working the Story In Review # PO will check to make sure the Story Acceptance Criteria is met Done # Acceptance Criteria has been verified successfully and the Story is complete User Stories are self-assigned by a team member who wants to accomplish the task. A User Story will move from a status of To Do \u2192 In Progress when it is the at the top of the Assignee's To Do list and they are ready to start. Kanban utilizes Work In Progress (WIP) limits to help identify bottlenecks and promoting moving tasks to Done . The CCPP has implemented the following WIP limits: Stalled : 4 In Progress : 6 In Review : 4","title":"Kanban Board"},{"location":"how-to/agile/#demonstrations","text":"As new functionality is rolled out we will be taking advantage of different platforms to demonstrate working products to our stakeholders. Feedback is strongly desired during these demonstrations in order to ensure what the team is delivering is accurately meeting the user requirements. These interactions make sure the team is continuously improving.","title":"Demonstrations"},{"location":"how-to/agile/#retrospectives","text":"Once a month the team will look back on the process overall. The focus will be on what has been working, what hasn't been working, and what improvements should we try in order to improve the process for our workflow.","title":"Retrospectives"},{"location":"how-to/build-docs/","text":"How to build this documentation 1. Make sure you have MkDocs installed pip install mkdocs 2. Clone the git repository git clone https://github.com/NCAR/cisl-cloud.git 3. Change in to docs/directory cd docs/ 4. Serve content with MkDocs mkdocs serve Documentation file structure mkdocs.yml # The configuration file. docs/ css/ extra.css # Contains extra sytling for the site. how-to/ agile.md # A description of the teams current Agile practices and policies build-docs.md # A how-to on building the documentation website images/ * # All custom images used in the site img/ favicon.ico # Overrides the default images used by readthedocs theme js/ clipboard.js # Javascript to copy code to the clipboard extra.js # Defining how to implement the clipboard copy in our HTML popper.min.js # Used to have the Copied pop up on click tippy-bundle.umd.js # Used to have the Copied pop up on click about.md # The project about trading contact.md # How to interact with the team features.md # A list of potential features for the project index.md # The documentation homepage. layout.md # Outline of site directories and files services.md # List of potential services we will offer","title":"Documentation Site Build"},{"location":"how-to/build-docs/#how-to-build-this-documentation","text":"","title":"How to build this documentation"},{"location":"how-to/build-docs/#1-make-sure-you-have-mkdocs-installed","text":"pip install mkdocs","title":"1.  Make sure you have MkDocs installed"},{"location":"how-to/build-docs/#2-clone-the-git-repository","text":"git clone https://github.com/NCAR/cisl-cloud.git","title":"2.  Clone the git repository"},{"location":"how-to/build-docs/#3-change-in-to-docsdirectory","text":"cd docs/","title":"3.  Change in to docs/directory"},{"location":"how-to/build-docs/#4-serve-content-with-mkdocs","text":"mkdocs serve","title":"4.  Serve content with MkDocs"},{"location":"how-to/build-docs/#documentation-file-structure","text":"mkdocs.yml # The configuration file. docs/ css/ extra.css # Contains extra sytling for the site. how-to/ agile.md # A description of the teams current Agile practices and policies build-docs.md # A how-to on building the documentation website images/ * # All custom images used in the site img/ favicon.ico # Overrides the default images used by readthedocs theme js/ clipboard.js # Javascript to copy code to the clipboard extra.js # Defining how to implement the clipboard copy in our HTML popper.min.js # Used to have the Copied pop up on click tippy-bundle.umd.js # Used to have the Copied pop up on click about.md # The project about trading contact.md # How to interact with the team features.md # A list of potential features for the project index.md # The documentation homepage. layout.md # Outline of site directories and files services.md # List of potential services we will offer","title":"Documentation file structure"},{"location":"how-to/k8sJH/customize-docker/","text":"How to Customize a Jupyter Docker Image for Single Users The Jupyter environment spun up for the users can be customized by building on top of the base Jupyter Docker image. For this use case the docker-stacks-foundation image from Jupyter is used. That image is built from ubuntu:22.04. This example Dockerfile has comments inline to explain what the lines are doing Dockerfile The following Dockerfile is built automatically anytime there is a push to the directory where it is stored via GitHub actions. Setting up a GitHub action to build and push a Docker image to Docker Hub can be viewed at the Setup GitHub Action section. # Borrowed heavily from the base-notebook Dockerfile by Jupyter # https://github.com/jupyter/docker-stacks/blob/main/base-notebook/Dockerfile # The shell scripts and python modules were developed by the Jupyter Development Team # This image provides a custom environment.yml and requirements.txt as well as # having some customizations injected into this Dockerfile # The base image used is the docker-stacks-foundation by Jupyter # https://github.com/jupyter/docker-stacks/blob/main/docker-stacks-foundation/Dockerfile FROM jupyter/docker-stacks-foundation:latest LABEL maintainer=\"CISL Cloud Pilot Team <cisl-cloud-pilot@ucar.edu>\" # Fix: https://github.com/hadolint/hadolint/wiki/DL4006 # Fix: https://github.com/koalaman/shellcheck/wiki/SC3014 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] USER root # Install all OS dependencies for notebook server that starts but lacks all # features (e.g., download as all possible file formats) RUN apt-get update --yes && \\ apt-get install --yes --no-install-recommends \\ fonts-liberation \\ curl \\ emacs \\ nodejs \\ npm \\ git \\ vim \\ # - pandoc is used to convert notebooks to html files # it's not present in aarch64 ubuntu image, so we install it here pandoc \\ # - run-one - a wrapper script that runs no more # than one unique instance of some command with a unique set of arguments, # we use `run-one-constantly` to support `RESTARTABLE` option run-one && \\ apt-get clean && rm -rf /var/lib/apt/lists/* USER ${NB_UID} # Install Jupyter Notebook, Lab, and Hub # Generate a notebook server config # Cleanup temporary files # Correct permissions # Do all this in a single RUN command to avoid duplicating all of the # files across image layers when the permissions change COPY configs/jupyter/base-notebook/environment.yml configs/jupyter/base-notebook/requirements.txt /tmp/ WORKDIR /tmp RUN mamba install --quiet --yes \\ # NodeJS >= 18.0 is required for `jupyter lab build` command # https://github.com/jupyter/docker-stacks/issues/1901 'nodejs>=18.0' \\ 'notebook' \\ 'jupyterhub' \\ 'jupyterlab==3.6.3' \\ 'conda-forge::nb_conda_kernels' && \\ # Pin NodeJS echo 'nodejs >=18.0' >> \"${CONDA_DIR}/conda-meta/pinned\" && \\ # nb_conda_kernels is required to save user environments as custom user notebook kernels that persist # Create a kernel named cisl-cloud-base from the environment.yml file mamba env update --name cisl-cloud-base -f environment.yml && \\ # Install nbgitpuller via pip pip install -r requirements.txt && \\ jupyter notebook --generate-config && \\ mamba clean --all -f -y && \\ npm cache clean --force && \\ jupyter lab clean && \\ mkdir -p .config/ && \\ mkdir -p .config/dask/ && \\ rm -rf \"/home/${NB_USER}/.cache/yarn\" && \\ fix-permissions \"${CONDA_DIR}\" && \\ fix-permissions \"/home/${NB_USER}\" ENV JUPYTER_PORT=8888 EXPOSE $JUPYTER_PORT # Configure container startup CMD [\"start-notebook.sh\"] # Copy local files as late as possible to avoid cache busting COPY configs/jupyter/base-notebook/scripts/start-notebook.sh configs/jupyter/base-notebook/scripts/start-singleuser.sh /usr/local/bin/ # Currently need to have both jupyter_notebook_config and jupyter_server_config to support classic and lab COPY configs/jupyter/base-notebook/scripts/jupyter_server_config.py configs/jupyter/base-notebook/scripts/docker_healthcheck.py /etc/jupyter/ # Fix permissions on /etc/jupyter as root USER root RUN rm -rf /tmp/environment.yml && \\ rm -rf /tmp/requirements.txt # Legacy for Jupyter Notebook Server, see: [#1205](https://github.com/jupyter/docker-stacks/issues/1205) RUN sed -re \"s/c.ServerApp/c.NotebookApp/g\" \\ /etc/jupyter/jupyter_server_config.py > /etc/jupyter/jupyter_notebook_config.py && \\ fix-permissions /etc/jupyter/ # Used to allow folder deletions RUN sed -i 's/c.FileContentsManager.delete_to_trash = False/c.FileContentsManager.always_delete_dir = True/g' /etc/jupyter/jupyter_server_config.py # HEALTHCHECK documentation: https://docs.docker.com/engine/reference/builder/#healthcheck # This healtcheck works well for `lab`, `notebook`, `nbclassic`, `server` and `retro` jupyter commands # https://github.com/jupyter/docker-stacks/issues/915#issuecomment-1068528799 HEALTHCHECK --interval=5s --timeout=3s --start-period=5s --retries=3 \\ CMD /etc/jupyter/docker_healthcheck.py || exit 1 # Copy the .condarc file to allow for saving of user custom conda environments COPY configs/jupyter/base-notebook/config/.condarc / COPY configs/jupyter/base-notebook/config/.condarc /opt/conda/ COPY configs/jupyter/base-notebook/config/.profile /.bash_profile COPY configs/jupyter/base-notebook/config/.bashrc / COPY configs/jupyter/base-notebook/config/.bashrc /etc/bash.bashrc COPY configs/jupyter/base-notebook/config/distributed.yml / # Switch back to jovyan to avoid accidental container runs as root USER ${NB_UID} WORKDIR \"${HOME}\" environment.yml The environment.yml file is where most of the python packages will be installed from via mamba. The following are the packages installed currently in the cisl-cloud-base conda kernel. name: cisl-cloud-base channels: - conda-forge dependencies: # Filesystem interface for Azure # https://pypi.org/project/adlfs/ - adlfs # Vega-Altair: A declarative statistical visualization library for Python. # https://pypi.org/project/altair/ - altair # Argo Data access # https://pypi.org/project/argopy/ - argopy # AWS Command Line Interface # https://pypi.org/project/awscli/ - awscli # Screen-scraping library # https://pypi.org/project/beautifulsoup4/ - beautifulsoup4 # Interactive plots and applications in the browser from Python # https://pypi.org/project/bokeh/ - bokeh # AWS Software Development Kit (SDK) # https://pypi.org/project/boto3/ - boto3 # Collection of fast NumPy array functions written in C # https://pypi.org/project/Bottleneck/ - bottleneck # Cartographic python library with Matplotlib support for visualization # https://pypi.org/project/Cartopy/0.21.1/ - cartopy # Interface to map GRIB files to NetCDF # https://pypi.org/project/cfgrib/ - cfgrib # Extended pickling support for Python objects # https://pypi.org/project/cloudpickle/ - cloudpickle # The Cython compiler for writing C extensions for the Python language. # https://pypi.org/project/Cython/ - cython # Parallel PyData with Task Scheduling # https://pypi.org/project/dask/ - dask # Library for interacting with a dask-gateway server # https://pypi.org/project/dask-gateway/ - dask-gateway # Library to deploy dask on HPC job queuing systems like PBS, Slurm, etc. # https://pypi.org/project/dask-jobqueue/ - dask-jobqueue # Use geometric objects as matplotlib paths and patches # https://pypi.org/project/descartes/ - descartes # serialize all of python # https://pypi.org/project/dill/ - dill # Client library for NASA Earthdata APIs # https://pypi.org/project/earthaccess/ - earthaccess # Empirical orthogonal function (EOF) analysis in Python # https://pypi.org/project/eofs/ - eofs # Python interface for ERDDAP # https://pypi.org/project/erddapy/ - erddapy # GroupBy operations for dask.array # https://pypi.org/project/flox/ - flox # File-system specification # https://pypi.org/project/fsspec/ - fsspec # Diffusion-based Spatial Filtering of Gridded Data # https://pypi.org/project/gcm-filters/ - gcm_filters # Filesystem interface for Google Cloud Storage # https://pypi.org/project/gcsfs/ - gcsfs # Tool to convert geopandas vector data into rasterized xarray data. # https://pypi.org/project/geocube/ - geocube # Geographic pandas extensions # https://pypi.org/project/geopandas/ - geopandas # Python Geocoding Toolbox # https://pypi.org/project/geopy/ - geopy # GeoViews is a Python library that makes it easy to explore and visualize geographical, meteorological, and oceanographic datasets # https://pypi.org/project/geoviews/ - geoviews-core # Temporary, well scoped credentials for pushing to GitHub # https://pypi.org/project/gh-scoped-creds/ - gh-scoped-creds # Gibbs Seawater Oceanographic Package of TEOS-10 # https://pypi.org/project/gsw/ - gsw # netCDF4 via h5py # https://pypi.org/project/h5netcdf/ - h5netcdf # Read and write HDF5 files from Python # https://pypi.org/project/h5py/ - h5py # Stop plotting your data - annotate your data and let it visualize itself. # https://pypi.org/project/holoviews/ - holoviews # A high-level plotting API for the PyData ecosystem built on HoloViews. # https://pypi.org/project/hvplot/ - hvplot # Data load and catalog system # https://pypi.org/project/intake/ - intake # An intake plugin for parsing an Earth System Model (ESM) catalog and loading netCDF files and/or Zarr stores into Xarray datasets. # https://pypi.org/project/intake-esm/ - intake-esm # xarray plugins for Intake # https://pypi.org/project/intake-xarray/ - intake-xarray # IPython Kernel for Jupyter # https://pypi.org/project/ipykernel/ - ipykernel>=6.21.2 # Matplotlib Jupyter Extension # https://pypi.org/project/ipympl/ - ipympl # Jupyter interactive widgets # https://pypi.org/project/ipywidgets/ - ipywidgets # Allows embedding of Jupyter widgets in Bokeh layouts. # https://pypi.org/project/ipywidgets-bokeh/ - ipywidgets-bokeh # A Jupyter extension for rendering Bokeh content. # https://pypi.org/project/jupyter-bokeh/ - jupyter_bokeh # Jupyter Server Proxy for Panel applications # https://pypi.org/project/jupyter-panel-proxy/ - jupyter-panel-proxy # Jupyter Extension to show resource usage # https://pypi.org/project/jupyter-resource-usage/ - jupyter-resource-usage # Functions to make reference descriptions for ReferenceFileSystem # https://pypi.org/project/kerchunk/ - kerchunk # Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API. # https://pypi.org/project/lxml/ - lxml # Python plotting package # https://pypi.org/project/matplotlib/ - matplotlib # Collection of tools for reading, visualizing and performing calculations with weather data. # https://pypi.org/project/MetPy/ - metpy # Strips outputs from Jupyter and IPython notebooks # https://pypi.org/project/nbstripout/ - nbstripout # Provides support for a cftime axis in matplotlib # https://pypi.org/project/nc-time-axis/ - nc-time-axis # Provides an object-oriented python interface to the netCDF version 4 library # https://pypi.org/project/netCDF4/ - netcdf4 # compiling Python code using LLVM # https://pypi.org/project/numba/ - numba # Fast numerical expression evaluator for NumPy # https://pypi.org/project/numexpr/ - numexpr # Fundamental package for array computing in Python # https://pypi.org/project/numpy/ - numpy # Tooling for converting STAC metadata to ODC data model # https://pypi.org/project/odc-stac/ - odc-stac # A Python library to read/write Excel 2010 xlsx/xlsm files # https://pypi.org/project/openpyxl/ - openpyxl # Powerful data structures for data analysis, time series, and statistics # https://pypi.org/project/pandas/ - pandas # The powerful data exploration & web app framework for Python. # https://pypi.org/project/panel/ - panel # A Python package for describing statistical models and for building design matrices. # https://pypi.org/project/patsy/ - patsy # Framework for Lagrangian tracking of virtual ocean particles in the petascale age. # https://pypi.org/project/parcels/ - parcels # Make your Python code clearer and more reliable by declaring Parameters. # https://pypi.org/project/param/ - param # Tools to support analysis of POP2-CESM model solutions with xarray # https://pypi.org/project/pop-tools/ - pop-tools # Protocol Buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data. # https://protobuf.dev/ - protobuf # An implementation of the Data Access Protocol. # https://pypi.org/project/pydap/ - pydap # Python library for working with Spatiotemporal Asset Catalog (STAC). # https://pypi.org/project/pystac/ - pystac # https://pypi.org/project/pystac-client/ - pystac-client # PyTables is a package for managing hierarchical datasets and designed to efficiently and easily cope with extremely large amounts of data # https://www.pytables.org/ - pytables # A framework for requesting AWIPS meteorological datasets from an EDEX server # https://pypi.org/project/python-awips/ - python-awips # Simple Python interface for Graphviz # https://pypi.org/project/graphviz/ - python-graphviz # Fast and direct raster I/O for use with Numpy and SciPy # https://pypi.org/project/rasterio/ - rasterio # A library for rechunking arrays. # https://pypi.org/project/rechunker/ - rechunker # Cloud Optimized GeoTIFF (COGEO) creation plugin for rasterio # https://pypi.org/project/rio-cogeo/ - rio-cogeo # geospatial xarray extension powered by rasterio # https://pypi.org/project/rioxarray/ - rioxarray # Convenient Filesystem interface over S3 # https://pypi.org/project/s3fs/ - s3fs # Python package for earth-observing satellite data processing # https://pypi.org/project/satpy/ - satpy # Image processing in Python # https://pypi.org/project/scikit-image/ - scikit-image # A set of python modules for machine learning and data mining # https://pypi.org/project/scikit-learn/ - scikit-learn # Fundamental algorithms for scientific computing in Python # https://pypi.org/project/scipy/ - scipy # Statistical data visualization # https://pypi.org/project/seaborn/ - seaborn # A collection of Python utilities for interacting with the Unidata technology stack. # https://pypi.org/project/siphon/ - siphon # A web-based viewer for Python profiler output # https://pypi.org/project/snakeviz/ - snakeviz # Sparse n-dimensional arrays # https://pypi.org/project/sparse/ - sparse # Database Abstraction Library # https://pypi.org/project/SQLAlchemy/ - sqlalchemy # Load a STAC collection into xarray with dask # https://pypi.org/project/stackstac/ - stackstac # Statistical computations and models for Python # https://pypi.org/project/statsmodels/ - statsmodels # Computer algebra system (CAS) in Python # https://pypi.org/project/sympy/ - sympy # Pythonic interface to the TileDB array storage manager # https://pypi.org/project/tiledb/ - tiledb-py # fast python package for finding the timezone of any point on earth (coordinates) offline # https://pypi.org/project/timezonefinder/ - timezonefinder # Jupyter interactive widgets for Jupyter Notebook # https://pypi.org/project/widgetsnbextension/ - widgetsnbextension # N-D labeled arrays and datasets in Python # https://pypi.org/project/xarray/ - xarray # A collection of various tools for data analysis built on top of xarray and xgcm # https://pypi.org/project/xarrayutils/ - xarrayutils # Hierarchical tree-like data structures for xarray # https://pypi.org/project/xarray-datatree/ - xarray-datatree # An xarray extension for map plotting # https://pypi.org/project/xarray_leaflet/ - xarray_leaflet # xarray-based spatial analysis tools # https://pypi.org/project/xarray-spatial/ - xarray-spatial # Batch generation from Xarray objects # https://pypi.org/project/xbatcher/ - xbatcher # Climate indices computation package based on Xarray. # https://pypi.org/project/xclim/ - xclim # Universal Regridder for Geospatial Data # https://pypi.org/project/xesmf/ - xesmf # General Circulation Model Postprocessing with xarray # https://pypi.org/project/xgcm/ - xgcm # Fast, flexible, label-aware histograms for numpy and xarray # https://pypi.org/project/xhistogram/ - xhistogram # Analysis ready CMIP6 data the easy way # https://pypi.org/project/xmip/ - xmip # Read MITgcm mds binary files into xarray # https://pypi.org/project/xmitgcm/ - xmitgcm # Publish Xarray Datasets via a REST API. # https://pypi.org/project/xpublish/ - xpublish # Discrete Fourier Transform with xarray # https://pypi.org/project/xrft/ - xrft # An implementation of chunked, compressed, N-dimensional arrays for Python # https://pypi.org/project/zarr/ - zarr # Library for developers to extract data from Microsoft Excel (tm) spreadsheet files # https://pypi.org/project/xlrd3/ - xlrd requirements.txt pip is used to install the following Jupyter extensions so they are available in the base image irregardless of the conda enviornment that is active. nbgitpuller jupyterlab-git jupyter-server-proxy dask-labextension","title":"Customize Jupyter Docker Image"},{"location":"how-to/k8sJH/customize-docker/#how-to-customize-a-jupyter-docker-image-for-single-users","text":"The Jupyter environment spun up for the users can be customized by building on top of the base Jupyter Docker image. For this use case the docker-stacks-foundation image from Jupyter is used. That image is built from ubuntu:22.04. This example Dockerfile has comments inline to explain what the lines are doing","title":"How to Customize a Jupyter Docker Image for Single Users"},{"location":"how-to/k8sJH/customize-docker/#dockerfile","text":"The following Dockerfile is built automatically anytime there is a push to the directory where it is stored via GitHub actions. Setting up a GitHub action to build and push a Docker image to Docker Hub can be viewed at the Setup GitHub Action section. # Borrowed heavily from the base-notebook Dockerfile by Jupyter # https://github.com/jupyter/docker-stacks/blob/main/base-notebook/Dockerfile # The shell scripts and python modules were developed by the Jupyter Development Team # This image provides a custom environment.yml and requirements.txt as well as # having some customizations injected into this Dockerfile # The base image used is the docker-stacks-foundation by Jupyter # https://github.com/jupyter/docker-stacks/blob/main/docker-stacks-foundation/Dockerfile FROM jupyter/docker-stacks-foundation:latest LABEL maintainer=\"CISL Cloud Pilot Team <cisl-cloud-pilot@ucar.edu>\" # Fix: https://github.com/hadolint/hadolint/wiki/DL4006 # Fix: https://github.com/koalaman/shellcheck/wiki/SC3014 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] USER root # Install all OS dependencies for notebook server that starts but lacks all # features (e.g., download as all possible file formats) RUN apt-get update --yes && \\ apt-get install --yes --no-install-recommends \\ fonts-liberation \\ curl \\ emacs \\ nodejs \\ npm \\ git \\ vim \\ # - pandoc is used to convert notebooks to html files # it's not present in aarch64 ubuntu image, so we install it here pandoc \\ # - run-one - a wrapper script that runs no more # than one unique instance of some command with a unique set of arguments, # we use `run-one-constantly` to support `RESTARTABLE` option run-one && \\ apt-get clean && rm -rf /var/lib/apt/lists/* USER ${NB_UID} # Install Jupyter Notebook, Lab, and Hub # Generate a notebook server config # Cleanup temporary files # Correct permissions # Do all this in a single RUN command to avoid duplicating all of the # files across image layers when the permissions change COPY configs/jupyter/base-notebook/environment.yml configs/jupyter/base-notebook/requirements.txt /tmp/ WORKDIR /tmp RUN mamba install --quiet --yes \\ # NodeJS >= 18.0 is required for `jupyter lab build` command # https://github.com/jupyter/docker-stacks/issues/1901 'nodejs>=18.0' \\ 'notebook' \\ 'jupyterhub' \\ 'jupyterlab==3.6.3' \\ 'conda-forge::nb_conda_kernels' && \\ # Pin NodeJS echo 'nodejs >=18.0' >> \"${CONDA_DIR}/conda-meta/pinned\" && \\ # nb_conda_kernels is required to save user environments as custom user notebook kernels that persist # Create a kernel named cisl-cloud-base from the environment.yml file mamba env update --name cisl-cloud-base -f environment.yml && \\ # Install nbgitpuller via pip pip install -r requirements.txt && \\ jupyter notebook --generate-config && \\ mamba clean --all -f -y && \\ npm cache clean --force && \\ jupyter lab clean && \\ mkdir -p .config/ && \\ mkdir -p .config/dask/ && \\ rm -rf \"/home/${NB_USER}/.cache/yarn\" && \\ fix-permissions \"${CONDA_DIR}\" && \\ fix-permissions \"/home/${NB_USER}\" ENV JUPYTER_PORT=8888 EXPOSE $JUPYTER_PORT # Configure container startup CMD [\"start-notebook.sh\"] # Copy local files as late as possible to avoid cache busting COPY configs/jupyter/base-notebook/scripts/start-notebook.sh configs/jupyter/base-notebook/scripts/start-singleuser.sh /usr/local/bin/ # Currently need to have both jupyter_notebook_config and jupyter_server_config to support classic and lab COPY configs/jupyter/base-notebook/scripts/jupyter_server_config.py configs/jupyter/base-notebook/scripts/docker_healthcheck.py /etc/jupyter/ # Fix permissions on /etc/jupyter as root USER root RUN rm -rf /tmp/environment.yml && \\ rm -rf /tmp/requirements.txt # Legacy for Jupyter Notebook Server, see: [#1205](https://github.com/jupyter/docker-stacks/issues/1205) RUN sed -re \"s/c.ServerApp/c.NotebookApp/g\" \\ /etc/jupyter/jupyter_server_config.py > /etc/jupyter/jupyter_notebook_config.py && \\ fix-permissions /etc/jupyter/ # Used to allow folder deletions RUN sed -i 's/c.FileContentsManager.delete_to_trash = False/c.FileContentsManager.always_delete_dir = True/g' /etc/jupyter/jupyter_server_config.py # HEALTHCHECK documentation: https://docs.docker.com/engine/reference/builder/#healthcheck # This healtcheck works well for `lab`, `notebook`, `nbclassic`, `server` and `retro` jupyter commands # https://github.com/jupyter/docker-stacks/issues/915#issuecomment-1068528799 HEALTHCHECK --interval=5s --timeout=3s --start-period=5s --retries=3 \\ CMD /etc/jupyter/docker_healthcheck.py || exit 1 # Copy the .condarc file to allow for saving of user custom conda environments COPY configs/jupyter/base-notebook/config/.condarc / COPY configs/jupyter/base-notebook/config/.condarc /opt/conda/ COPY configs/jupyter/base-notebook/config/.profile /.bash_profile COPY configs/jupyter/base-notebook/config/.bashrc / COPY configs/jupyter/base-notebook/config/.bashrc /etc/bash.bashrc COPY configs/jupyter/base-notebook/config/distributed.yml / # Switch back to jovyan to avoid accidental container runs as root USER ${NB_UID} WORKDIR \"${HOME}\"","title":"Dockerfile"},{"location":"how-to/k8sJH/customize-docker/#environmentyml","text":"The environment.yml file is where most of the python packages will be installed from via mamba. The following are the packages installed currently in the cisl-cloud-base conda kernel. name: cisl-cloud-base channels: - conda-forge dependencies: # Filesystem interface for Azure # https://pypi.org/project/adlfs/ - adlfs # Vega-Altair: A declarative statistical visualization library for Python. # https://pypi.org/project/altair/ - altair # Argo Data access # https://pypi.org/project/argopy/ - argopy # AWS Command Line Interface # https://pypi.org/project/awscli/ - awscli # Screen-scraping library # https://pypi.org/project/beautifulsoup4/ - beautifulsoup4 # Interactive plots and applications in the browser from Python # https://pypi.org/project/bokeh/ - bokeh # AWS Software Development Kit (SDK) # https://pypi.org/project/boto3/ - boto3 # Collection of fast NumPy array functions written in C # https://pypi.org/project/Bottleneck/ - bottleneck # Cartographic python library with Matplotlib support for visualization # https://pypi.org/project/Cartopy/0.21.1/ - cartopy # Interface to map GRIB files to NetCDF # https://pypi.org/project/cfgrib/ - cfgrib # Extended pickling support for Python objects # https://pypi.org/project/cloudpickle/ - cloudpickle # The Cython compiler for writing C extensions for the Python language. # https://pypi.org/project/Cython/ - cython # Parallel PyData with Task Scheduling # https://pypi.org/project/dask/ - dask # Library for interacting with a dask-gateway server # https://pypi.org/project/dask-gateway/ - dask-gateway # Library to deploy dask on HPC job queuing systems like PBS, Slurm, etc. # https://pypi.org/project/dask-jobqueue/ - dask-jobqueue # Use geometric objects as matplotlib paths and patches # https://pypi.org/project/descartes/ - descartes # serialize all of python # https://pypi.org/project/dill/ - dill # Client library for NASA Earthdata APIs # https://pypi.org/project/earthaccess/ - earthaccess # Empirical orthogonal function (EOF) analysis in Python # https://pypi.org/project/eofs/ - eofs # Python interface for ERDDAP # https://pypi.org/project/erddapy/ - erddapy # GroupBy operations for dask.array # https://pypi.org/project/flox/ - flox # File-system specification # https://pypi.org/project/fsspec/ - fsspec # Diffusion-based Spatial Filtering of Gridded Data # https://pypi.org/project/gcm-filters/ - gcm_filters # Filesystem interface for Google Cloud Storage # https://pypi.org/project/gcsfs/ - gcsfs # Tool to convert geopandas vector data into rasterized xarray data. # https://pypi.org/project/geocube/ - geocube # Geographic pandas extensions # https://pypi.org/project/geopandas/ - geopandas # Python Geocoding Toolbox # https://pypi.org/project/geopy/ - geopy # GeoViews is a Python library that makes it easy to explore and visualize geographical, meteorological, and oceanographic datasets # https://pypi.org/project/geoviews/ - geoviews-core # Temporary, well scoped credentials for pushing to GitHub # https://pypi.org/project/gh-scoped-creds/ - gh-scoped-creds # Gibbs Seawater Oceanographic Package of TEOS-10 # https://pypi.org/project/gsw/ - gsw # netCDF4 via h5py # https://pypi.org/project/h5netcdf/ - h5netcdf # Read and write HDF5 files from Python # https://pypi.org/project/h5py/ - h5py # Stop plotting your data - annotate your data and let it visualize itself. # https://pypi.org/project/holoviews/ - holoviews # A high-level plotting API for the PyData ecosystem built on HoloViews. # https://pypi.org/project/hvplot/ - hvplot # Data load and catalog system # https://pypi.org/project/intake/ - intake # An intake plugin for parsing an Earth System Model (ESM) catalog and loading netCDF files and/or Zarr stores into Xarray datasets. # https://pypi.org/project/intake-esm/ - intake-esm # xarray plugins for Intake # https://pypi.org/project/intake-xarray/ - intake-xarray # IPython Kernel for Jupyter # https://pypi.org/project/ipykernel/ - ipykernel>=6.21.2 # Matplotlib Jupyter Extension # https://pypi.org/project/ipympl/ - ipympl # Jupyter interactive widgets # https://pypi.org/project/ipywidgets/ - ipywidgets # Allows embedding of Jupyter widgets in Bokeh layouts. # https://pypi.org/project/ipywidgets-bokeh/ - ipywidgets-bokeh # A Jupyter extension for rendering Bokeh content. # https://pypi.org/project/jupyter-bokeh/ - jupyter_bokeh # Jupyter Server Proxy for Panel applications # https://pypi.org/project/jupyter-panel-proxy/ - jupyter-panel-proxy # Jupyter Extension to show resource usage # https://pypi.org/project/jupyter-resource-usage/ - jupyter-resource-usage # Functions to make reference descriptions for ReferenceFileSystem # https://pypi.org/project/kerchunk/ - kerchunk # Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API. # https://pypi.org/project/lxml/ - lxml # Python plotting package # https://pypi.org/project/matplotlib/ - matplotlib # Collection of tools for reading, visualizing and performing calculations with weather data. # https://pypi.org/project/MetPy/ - metpy # Strips outputs from Jupyter and IPython notebooks # https://pypi.org/project/nbstripout/ - nbstripout # Provides support for a cftime axis in matplotlib # https://pypi.org/project/nc-time-axis/ - nc-time-axis # Provides an object-oriented python interface to the netCDF version 4 library # https://pypi.org/project/netCDF4/ - netcdf4 # compiling Python code using LLVM # https://pypi.org/project/numba/ - numba # Fast numerical expression evaluator for NumPy # https://pypi.org/project/numexpr/ - numexpr # Fundamental package for array computing in Python # https://pypi.org/project/numpy/ - numpy # Tooling for converting STAC metadata to ODC data model # https://pypi.org/project/odc-stac/ - odc-stac # A Python library to read/write Excel 2010 xlsx/xlsm files # https://pypi.org/project/openpyxl/ - openpyxl # Powerful data structures for data analysis, time series, and statistics # https://pypi.org/project/pandas/ - pandas # The powerful data exploration & web app framework for Python. # https://pypi.org/project/panel/ - panel # A Python package for describing statistical models and for building design matrices. # https://pypi.org/project/patsy/ - patsy # Framework for Lagrangian tracking of virtual ocean particles in the petascale age. # https://pypi.org/project/parcels/ - parcels # Make your Python code clearer and more reliable by declaring Parameters. # https://pypi.org/project/param/ - param # Tools to support analysis of POP2-CESM model solutions with xarray # https://pypi.org/project/pop-tools/ - pop-tools # Protocol Buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data. # https://protobuf.dev/ - protobuf # An implementation of the Data Access Protocol. # https://pypi.org/project/pydap/ - pydap # Python library for working with Spatiotemporal Asset Catalog (STAC). # https://pypi.org/project/pystac/ - pystac # https://pypi.org/project/pystac-client/ - pystac-client # PyTables is a package for managing hierarchical datasets and designed to efficiently and easily cope with extremely large amounts of data # https://www.pytables.org/ - pytables # A framework for requesting AWIPS meteorological datasets from an EDEX server # https://pypi.org/project/python-awips/ - python-awips # Simple Python interface for Graphviz # https://pypi.org/project/graphviz/ - python-graphviz # Fast and direct raster I/O for use with Numpy and SciPy # https://pypi.org/project/rasterio/ - rasterio # A library for rechunking arrays. # https://pypi.org/project/rechunker/ - rechunker # Cloud Optimized GeoTIFF (COGEO) creation plugin for rasterio # https://pypi.org/project/rio-cogeo/ - rio-cogeo # geospatial xarray extension powered by rasterio # https://pypi.org/project/rioxarray/ - rioxarray # Convenient Filesystem interface over S3 # https://pypi.org/project/s3fs/ - s3fs # Python package for earth-observing satellite data processing # https://pypi.org/project/satpy/ - satpy # Image processing in Python # https://pypi.org/project/scikit-image/ - scikit-image # A set of python modules for machine learning and data mining # https://pypi.org/project/scikit-learn/ - scikit-learn # Fundamental algorithms for scientific computing in Python # https://pypi.org/project/scipy/ - scipy # Statistical data visualization # https://pypi.org/project/seaborn/ - seaborn # A collection of Python utilities for interacting with the Unidata technology stack. # https://pypi.org/project/siphon/ - siphon # A web-based viewer for Python profiler output # https://pypi.org/project/snakeviz/ - snakeviz # Sparse n-dimensional arrays # https://pypi.org/project/sparse/ - sparse # Database Abstraction Library # https://pypi.org/project/SQLAlchemy/ - sqlalchemy # Load a STAC collection into xarray with dask # https://pypi.org/project/stackstac/ - stackstac # Statistical computations and models for Python # https://pypi.org/project/statsmodels/ - statsmodels # Computer algebra system (CAS) in Python # https://pypi.org/project/sympy/ - sympy # Pythonic interface to the TileDB array storage manager # https://pypi.org/project/tiledb/ - tiledb-py # fast python package for finding the timezone of any point on earth (coordinates) offline # https://pypi.org/project/timezonefinder/ - timezonefinder # Jupyter interactive widgets for Jupyter Notebook # https://pypi.org/project/widgetsnbextension/ - widgetsnbextension # N-D labeled arrays and datasets in Python # https://pypi.org/project/xarray/ - xarray # A collection of various tools for data analysis built on top of xarray and xgcm # https://pypi.org/project/xarrayutils/ - xarrayutils # Hierarchical tree-like data structures for xarray # https://pypi.org/project/xarray-datatree/ - xarray-datatree # An xarray extension for map plotting # https://pypi.org/project/xarray_leaflet/ - xarray_leaflet # xarray-based spatial analysis tools # https://pypi.org/project/xarray-spatial/ - xarray-spatial # Batch generation from Xarray objects # https://pypi.org/project/xbatcher/ - xbatcher # Climate indices computation package based on Xarray. # https://pypi.org/project/xclim/ - xclim # Universal Regridder for Geospatial Data # https://pypi.org/project/xesmf/ - xesmf # General Circulation Model Postprocessing with xarray # https://pypi.org/project/xgcm/ - xgcm # Fast, flexible, label-aware histograms for numpy and xarray # https://pypi.org/project/xhistogram/ - xhistogram # Analysis ready CMIP6 data the easy way # https://pypi.org/project/xmip/ - xmip # Read MITgcm mds binary files into xarray # https://pypi.org/project/xmitgcm/ - xmitgcm # Publish Xarray Datasets via a REST API. # https://pypi.org/project/xpublish/ - xpublish # Discrete Fourier Transform with xarray # https://pypi.org/project/xrft/ - xrft # An implementation of chunked, compressed, N-dimensional arrays for Python # https://pypi.org/project/zarr/ - zarr # Library for developers to extract data from Microsoft Excel (tm) spreadsheet files # https://pypi.org/project/xlrd3/ - xlrd","title":"environment.yml"},{"location":"how-to/k8sJH/customize-docker/#requirementstxt","text":"pip is used to install the following Jupyter extensions so they are available in the base image irregardless of the conda enviornment that is active. nbgitpuller jupyterlab-git jupyter-server-proxy dask-labextension","title":"requirements.txt"},{"location":"how-to/k8sJH/install/","text":"Install JupyterHub on k8s","title":"Install JupyterHub on k8s"},{"location":"how-to/k8sJH/install/#install-jupyterhub-on-k8s","text":"","title":"Install JupyterHub on k8s"},{"location":"how-to/k8sJH/setup-gh-act/","text":"Setup GitHub actions to Build Docker image automatically As part of having a continuous integration workflow GitHub actions is utilized to rebuild Docker images whenever new code is pushed to a specific directory. In order to utilize GitHub actions a directory named .github needs to be created in the base directory of the repository. The .github directory should also contain a directory named workflows in order to store and run the different workflows created. The workflow itself is a yaml file that defines the actions that trigger and execute what is required for the continuous integration application. Example .github/workflows/build-push-basenb.yaml This example workflow utilizes a few different github actions to build a docker image and push it to a container registry. Each step is explained in detail via inline comments. # This workflow builds docker images and pushes them to a Docker Hub Repository # This workflow is specific to the base-notebook directory and image # Set the workflow name name: Deploy base-notebook # Define the trigger that starts the action # For this workflow the trigger is on a push that changes anything in the configs/jupyter/base-notebook/ path on: push: paths: - configs/jupyter/base-notebook/** # Define the actions that are going to take place as part of this workflow jobs: # Name the job(s) deploy-docker-base-notebook: # Define where the job should run in this case it will be run on the latest ubuntu image runs-on: ubuntu-latest # Set the steps to take in order steps: # Step 1 is to checkout the github repo used to build the Dockerfile - name: Check out the repo uses: actions/checkout@v3 # Step 2 is to login to docker hub so the image can be pushed - name: Login to Docker Hub uses: docker/login-action@v2 # GitHub secrets are used to provide login information to docker hub with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} # Pull relevant metadata out of the docker image used # This can be expanded upon - name: Extract metadata for Docker id: meta uses: docker/metadata-action@v4 with: images: ncote/cisl-cloud-base # Build and push the docker image - name: Build and push Docker image uses: docker/build-push-action@v4 with: # Provide the current directory as build context context: . # Specify where the Dockerfile is located in relation to the repo base path file: configs/jupyter/base-notebook/Dockerfile # Enable the push to docker hub push: true # Provide the tags to apply to the image, this example uses the latest image tag tags: ncote/cisl-cloud-base:latest # Apply labels as defined in the Docker image metadata labels: ${{ steps.meta.outputs.labels }}","title":"Setup GitHub Action to Build and Push Docker Image"},{"location":"how-to/k8sJH/setup-gh-act/#setup-github-actions-to-build-docker-image-automatically","text":"As part of having a continuous integration workflow GitHub actions is utilized to rebuild Docker images whenever new code is pushed to a specific directory. In order to utilize GitHub actions a directory named .github needs to be created in the base directory of the repository. The .github directory should also contain a directory named workflows in order to store and run the different workflows created. The workflow itself is a yaml file that defines the actions that trigger and execute what is required for the continuous integration application.","title":"Setup GitHub actions to Build Docker image automatically"},{"location":"how-to/k8sJH/setup-gh-act/#example-githubworkflowsbuild-push-basenbyaml","text":"This example workflow utilizes a few different github actions to build a docker image and push it to a container registry. Each step is explained in detail via inline comments. # This workflow builds docker images and pushes them to a Docker Hub Repository # This workflow is specific to the base-notebook directory and image # Set the workflow name name: Deploy base-notebook # Define the trigger that starts the action # For this workflow the trigger is on a push that changes anything in the configs/jupyter/base-notebook/ path on: push: paths: - configs/jupyter/base-notebook/** # Define the actions that are going to take place as part of this workflow jobs: # Name the job(s) deploy-docker-base-notebook: # Define where the job should run in this case it will be run on the latest ubuntu image runs-on: ubuntu-latest # Set the steps to take in order steps: # Step 1 is to checkout the github repo used to build the Dockerfile - name: Check out the repo uses: actions/checkout@v3 # Step 2 is to login to docker hub so the image can be pushed - name: Login to Docker Hub uses: docker/login-action@v2 # GitHub secrets are used to provide login information to docker hub with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} # Pull relevant metadata out of the docker image used # This can be expanded upon - name: Extract metadata for Docker id: meta uses: docker/metadata-action@v4 with: images: ncote/cisl-cloud-base # Build and push the docker image - name: Build and push Docker image uses: docker/build-push-action@v4 with: # Provide the current directory as build context context: . # Specify where the Dockerfile is located in relation to the repo base path file: configs/jupyter/base-notebook/Dockerfile # Enable the push to docker hub push: true # Provide the tags to apply to the image, this example uses the latest image tag tags: ncote/cisl-cloud-base:latest # Apply labels as defined in the Docker image metadata labels: ${{ steps.meta.outputs.labels }}","title":"Example .github/workflows/build-push-basenb.yaml"},{"location":"overview/about/","text":"About this Project CISL is currently deploying a pilot on-premise prototype cloud environment for compute and storage. 2i2c deployed a JupyterHub instance in AWS. We would utilize this experience to leverage 2i2c knowledge for our own education. This instance can be utilized to run calculated tests to help determine costs associated with users running on AWS. On-premise cloud An on-premise (on-prem) cloud consists of storage, compute, and networking resources hosted on fully redundant hardware installed in personal/organizational facilities available to users Kubernetes (k8s) We will utilize a k8s cluster to host JupyterHub. Dask will be installed to enable parallel computing. A JupyterHub Spawner will create single user environments with access to a shared and personal storage space. The Spawned user environments will come in different sizes with a GPU option. k8s can also be used to host containers or containerized virtual machines for individual use cases. JupyterHub on k8s There is a JupyterHub instance hosted on-prem at NWSC on a RKE2 provisioned k8s cluster. This JupyterHub is going to have a customized Docker image that enables packages, kernels, and extensions the scientific research community utilizes to increase productivity in data analysis. The custom environment will also provide users read-only access to the campaign and collections directories on GLADE as well as a shared directory whose specific use case is still being fleshed out. Access to this JupyterHub will be handled via GitHub authentication and a team under the NCAR organization in GitHub. Storage GLADE NFS will be utilized to provide RO only access to GLADE on the Spawned JupyterHub user environments. Currently the collections and campaign directories on GLADE are available as read only. STRATUS S3 will be provided via CISLs object storage platform STRATUS . 2i2c JupyterHub 2i2c deployed a JupyterHub instance in AWS. Access to this JupyterHub instance is provided by GitHub Teams . Costs will be incurred for any AWS compute resources utilized by users. At this point in time the 2i2c deployed JupyterHub will be used to validate the 2i2c notebook configuration for the research community users. These validations will be orchestrated to develop an estimate of potential costs to use at scale. Storage Data Storage for the 2i2c JupyterHub instance is provided by AWS Elastic File System ( EFS ) Data Access AWS S3 Open Data Registry utilizes AWS S3 API calls the same way as STRATUS. By utilizing S3 API calls we can make Data accessible in a familiar way on the Web and on-premise. Agile Program Management Kanban Board This project is implementing a hybrid Agile Project Management workflow. Waterfall techniques will be used for high level project management. Kanban will be used for day to day tasks and creating a continuous flow of value to users.","title":"About"},{"location":"overview/about/#about-this-project","text":"CISL is currently deploying a pilot on-premise prototype cloud environment for compute and storage. 2i2c deployed a JupyterHub instance in AWS. We would utilize this experience to leverage 2i2c knowledge for our own education. This instance can be utilized to run calculated tests to help determine costs associated with users running on AWS.","title":"About this Project"},{"location":"overview/about/#on-premise-cloud","text":"An on-premise (on-prem) cloud consists of storage, compute, and networking resources hosted on fully redundant hardware installed in personal/organizational facilities available to users","title":"On-premise cloud"},{"location":"overview/about/#kubernetes-k8s","text":"We will utilize a k8s cluster to host JupyterHub. Dask will be installed to enable parallel computing. A JupyterHub Spawner will create single user environments with access to a shared and personal storage space. The Spawned user environments will come in different sizes with a GPU option. k8s can also be used to host containers or containerized virtual machines for individual use cases.","title":"Kubernetes (k8s)"},{"location":"overview/about/#jupyterhub-on-k8s","text":"There is a JupyterHub instance hosted on-prem at NWSC on a RKE2 provisioned k8s cluster. This JupyterHub is going to have a customized Docker image that enables packages, kernels, and extensions the scientific research community utilizes to increase productivity in data analysis. The custom environment will also provide users read-only access to the campaign and collections directories on GLADE as well as a shared directory whose specific use case is still being fleshed out. Access to this JupyterHub will be handled via GitHub authentication and a team under the NCAR organization in GitHub.","title":"JupyterHub on k8s"},{"location":"overview/about/#storage","text":"","title":"Storage"},{"location":"overview/about/#glade","text":"NFS will be utilized to provide RO only access to GLADE on the Spawned JupyterHub user environments. Currently the collections and campaign directories on GLADE are available as read only.","title":"GLADE"},{"location":"overview/about/#stratus","text":"S3 will be provided via CISLs object storage platform STRATUS .","title":"STRATUS"},{"location":"overview/about/#2i2c","text":"","title":"2i2c"},{"location":"overview/about/#jupyterhub","text":"2i2c deployed a JupyterHub instance in AWS. Access to this JupyterHub instance is provided by GitHub Teams . Costs will be incurred for any AWS compute resources utilized by users. At this point in time the 2i2c deployed JupyterHub will be used to validate the 2i2c notebook configuration for the research community users. These validations will be orchestrated to develop an estimate of potential costs to use at scale.","title":"JupyterHub"},{"location":"overview/about/#storage_1","text":"Data Storage for the 2i2c JupyterHub instance is provided by AWS Elastic File System ( EFS )","title":"Storage"},{"location":"overview/about/#data-access","text":"AWS S3 Open Data Registry utilizes AWS S3 API calls the same way as STRATUS. By utilizing S3 API calls we can make Data accessible in a familiar way on the Web and on-premise.","title":"Data Access"},{"location":"overview/about/#agile-program-management","text":"Kanban Board This project is implementing a hybrid Agile Project Management workflow. Waterfall techniques will be used for high level project management. Kanban will be used for day to day tasks and creating a continuous flow of value to users.","title":"Agile Program Management"},{"location":"overview/availability/","text":"Available Services and Status JupyterHub On-premise STATUS: In Development URL : https://jupyter.k8s.ucar.edu/ An on-premise JupyterHub is up and running, but is still in development. Authentication against github is being worked on currently but there is no authentication presently. The user space that is spun up has access to a shared NFS volume that is read-only as well as GLADE collections and campaign directories also mounted as read-only. The user notebook that is deployed is based on a custom Docker image that the CCPP team maintains. Documentation on how this was setup and deployed can be found on the how-to page in this documentation. AWS instance supported by 2i2c STATUS: In Testing URL : https://ncar-cisl.2i2c.cloud/ The JupyterHub instance setup and managed by 2i2c that runs on AWS is up and ready to use. Access to this JupyterHub is controlled via a GitHub team, specifically the NCAR organizations 2i2c-cloud-users team. Virtualization Kubernetes (k8s) We have a kubernetes cluster that we can utilize to host containers. We can provide users a private namespace to deploy to. We can also provision full kubernetes clusters for users via Rancher. Users would be administrators of their own k8s clusters but would have more freedom to customize to their needs and requirements. Virtual Machines (VMs) We can provide VMs to users as needed for tasks that aren't well suited for running in a container. Storage NFS We can provide general shared storage in the form of an NFS volume. Access to these systems is restricted by IP address and mounting is limited to on-premise machines. NFS mounting across wide area networks is not recommended. Object Storage Object Storage is available and our admins can create new buckets and assign user permissions for S3 interactions. Network Services Our systems will have routable IP addresses assigned and configured via DHCP. DNS records can also be added to provide full name resolution for systems provided.","title":"Current Offering"},{"location":"overview/availability/#available-services-and-status","text":"","title":"Available Services and Status"},{"location":"overview/availability/#jupyterhub","text":"","title":"JupyterHub"},{"location":"overview/availability/#on-premise","text":"STATUS: In Development URL : https://jupyter.k8s.ucar.edu/ An on-premise JupyterHub is up and running, but is still in development. Authentication against github is being worked on currently but there is no authentication presently. The user space that is spun up has access to a shared NFS volume that is read-only as well as GLADE collections and campaign directories also mounted as read-only. The user notebook that is deployed is based on a custom Docker image that the CCPP team maintains. Documentation on how this was setup and deployed can be found on the how-to page in this documentation.","title":"On-premise"},{"location":"overview/availability/#aws-instance-supported-by-2i2c","text":"STATUS: In Testing URL : https://ncar-cisl.2i2c.cloud/ The JupyterHub instance setup and managed by 2i2c that runs on AWS is up and ready to use. Access to this JupyterHub is controlled via a GitHub team, specifically the NCAR organizations 2i2c-cloud-users team.","title":"AWS instance supported by 2i2c"},{"location":"overview/availability/#virtualization","text":"","title":"Virtualization"},{"location":"overview/availability/#kubernetes-k8s","text":"We have a kubernetes cluster that we can utilize to host containers. We can provide users a private namespace to deploy to. We can also provision full kubernetes clusters for users via Rancher. Users would be administrators of their own k8s clusters but would have more freedom to customize to their needs and requirements.","title":"Kubernetes (k8s)"},{"location":"overview/availability/#virtual-machines-vms","text":"We can provide VMs to users as needed for tasks that aren't well suited for running in a container.","title":"Virtual Machines (VMs)"},{"location":"overview/availability/#storage","text":"","title":"Storage"},{"location":"overview/availability/#nfs","text":"We can provide general shared storage in the form of an NFS volume. Access to these systems is restricted by IP address and mounting is limited to on-premise machines. NFS mounting across wide area networks is not recommended.","title":"NFS"},{"location":"overview/availability/#object-storage","text":"Object Storage is available and our admins can create new buckets and assign user permissions for S3 interactions.","title":"Object Storage"},{"location":"overview/availability/#network-services","text":"Our systems will have routable IP addresses assigned and configured via DHCP. DNS records can also be added to provide full name resolution for systems provided.","title":"Network Services"},{"location":"overview/contact/","text":"Contact Us New requests Any new request should be submitted via a Jira Ticket . It will then follow our Kanban workflow . Technical Product Owner The Product Owner (PO), currently Nick Cote , is available via email or Google Chat to discuss any other needs you may have. Interacting with key stakeholders is the primary focus of the PO. They are more than happy to work with you as the primary interface for the technical team. Team Email address You can email all members of the team via cisl-on-prem-cloud@ucar.edu GitHub We have a private GitHub repository to host all of the configuration files and documentation outlined here. It is part of our internal continuous improvement process. It will be eventually opened up for more collaborative efforts.","title":"Contact Us"},{"location":"overview/contact/#contact-us","text":"","title":"Contact Us"},{"location":"overview/contact/#new-requests","text":"Any new request should be submitted via a Jira Ticket . It will then follow our Kanban workflow .","title":"New requests"},{"location":"overview/contact/#technical-product-owner","text":"The Product Owner (PO), currently Nick Cote , is available via email or Google Chat to discuss any other needs you may have. Interacting with key stakeholders is the primary focus of the PO. They are more than happy to work with you as the primary interface for the technical team.","title":"Technical Product Owner"},{"location":"overview/contact/#team-email-address","text":"You can email all members of the team via cisl-on-prem-cloud@ucar.edu","title":"Team Email address"},{"location":"overview/contact/#github","text":"We have a private GitHub repository to host all of the configuration files and documentation outlined here. It is part of our internal continuous improvement process. It will be eventually opened up for more collaborative efforts.","title":"GitHub"},{"location":"overview/features/","text":"Potential CCPP Features JupyterHub On-Prem 2i2c Provided AWS Instance BinderHub Web Hosting Tutorials Data Access Logging/Metrics Documentation Backups/DR RFE\u2019s SLA\u2019s","title":"Potential Features"},{"location":"overview/features/#potential-ccpp-features","text":"","title":"Potential CCPP Features"},{"location":"overview/features/#jupyterhub","text":"","title":"JupyterHub"},{"location":"overview/features/#on-prem","text":"","title":"On-Prem"},{"location":"overview/features/#2i2c-provided-aws-instance","text":"","title":"2i2c Provided AWS Instance"},{"location":"overview/features/#binderhub","text":"","title":"BinderHub"},{"location":"overview/features/#web-hosting","text":"","title":"Web Hosting"},{"location":"overview/features/#tutorials","text":"","title":"Tutorials"},{"location":"overview/features/#data-access","text":"","title":"Data Access"},{"location":"overview/features/#loggingmetrics","text":"","title":"Logging/Metrics"},{"location":"overview/features/#documentation","text":"","title":"Documentation"},{"location":"overview/features/#backupsdr","text":"","title":"Backups/DR"},{"location":"overview/features/#rfes","text":"","title":"RFE\u2019s"},{"location":"overview/features/#slas","text":"","title":"SLA\u2019s"},{"location":"overview/hw-resources/","text":"Hardware Resources Initial Resources (5/1/23) Compute Resources (5 Nodes) System Information Node Specifications Manufacturer Supermicro Model SYS-120U-TNR CPU Type Intel Xeon Gold 6326 CPU Speed 2.90 GHz CPU Cores 16 RAM (GB) 512 GPU Model Nvidia A2 Tensor GPU Cores 1280 GPU Memory 16 GB NICs 2x10G & 4x25G Storage 2x100GB & 6x1.6TB NVMe Totals CPU Cores RAM GPU Cores GPU Mem Local Storage 80 2.5 TB 6400 80 GB 48 TB Storage Resources STRATUS GLADE (RO) NFS LOCAL 3.3 PB 38 PB 110 TB 48 TB Network Resources","title":"Hardware"},{"location":"overview/hw-resources/#hardware-resources","text":"","title":"Hardware Resources"},{"location":"overview/hw-resources/#initial-resources-5123","text":"","title":"Initial Resources (5/1/23)"},{"location":"overview/hw-resources/#compute-resources-5-nodes","text":"System Information Node Specifications Manufacturer Supermicro Model SYS-120U-TNR CPU Type Intel Xeon Gold 6326 CPU Speed 2.90 GHz CPU Cores 16 RAM (GB) 512 GPU Model Nvidia A2 Tensor GPU Cores 1280 GPU Memory 16 GB NICs 2x10G & 4x25G Storage 2x100GB & 6x1.6TB NVMe","title":"Compute Resources (5 Nodes)"},{"location":"overview/hw-resources/#totals","text":"CPU Cores RAM GPU Cores GPU Mem Local Storage 80 2.5 TB 6400 80 GB 48 TB","title":"Totals"},{"location":"overview/hw-resources/#storage-resources","text":"STRATUS GLADE (RO) NFS LOCAL 3.3 PB 38 PB 110 TB 48 TB","title":"Storage Resources"},{"location":"overview/hw-resources/#network-resources","text":"","title":"Network Resources"},{"location":"overview/services/","text":"Potential Services offered JupyterHub On-Prem Jupyter notebook servers with multiple pre-determined resources. Web hosting for authorized access. 2i2c Provided AWS Instance Jupyter notebook servers with multiple pre-determined resources. Web hosting for authorized access. BinderHub Users get custom links to recreate environments on CISL on-premise or 2i2c JupyterHub. Provide a public or private Container Registry to host images created from GitHub repository. Web Hosting Visualization Documentation Offer dynamic or static web hosting. Provide unique publicly resolvable DNS names as defined by the user. Tutorials Control student access to available resources. Provide students with identical base image to level set starting point. Provision and tear down student workspaces in a quick and user friendly fashion. Allow console sharing & viewing to assist instructor troubleshooting. Data Access GLADE Read Only A read only NFS mount will be provided for us to utilize in providing access to datasets host on GLADE. STRATUS S3 buckets We will be Administrators for allocated space on STRAUS and will manage buckets and users. API access to buckets in the same fashion as AWS provide a familiar interface. Buckets can be used to host terraform state files if we decide to go that route. API We can utilize STRATUS and other offerings to create APIs to interact programmatically with datasets. File Sharing Offer a central location where model output and other data that has historically been hard to transfer can be served more easily to the users preferred location. Logging/Metrics Provide interactive dashboards for compute, storage, and networking resources. Log all user interactions with our services in order to provide utilization and usage numbers. Make a determination for on-premise service feasibility based on the information collected. Performance on the teams ability to meet Service Level Agreements (SLAs) will be recorded. Documentation Overall vision and goals will be presented as the project evolves to provide full visibility in to the objectives. Implementation of all systems will be documented with code so that someone unfamiliar with the project could recreate the environment. Interactions with cloud offerings will be documented fully so that end users know what services are offered and how to obtain or use those services. Service Level Agreements (SLAs) will be provided so that users know what level of service they can expect with their implementation. Provide details on Agile implementation and how to work with the team via that implementation. Service Level Agreements (SLAs) Defines and documents the operating procedures and commitments between providers and users. SLAs will be defined and documented for each service so users and maintainers can operate with the same expectations. Backups/Disaster Recovery (DR) Each supported service will have an associated and documented Backup and DR policy as part of the SLAs. Request For Enhancement (RFEs) Users will have an avenue for submitting RFEs and that process will be documented.","title":"Potential Services"},{"location":"overview/services/#potential-services-offered","text":"","title":"Potential Services offered"},{"location":"overview/services/#jupyterhub","text":"","title":"JupyterHub"},{"location":"overview/services/#on-prem","text":"Jupyter notebook servers with multiple pre-determined resources. Web hosting for authorized access.","title":"On-Prem"},{"location":"overview/services/#2i2c-provided-aws-instance","text":"Jupyter notebook servers with multiple pre-determined resources. Web hosting for authorized access.","title":"2i2c Provided AWS Instance"},{"location":"overview/services/#binderhub","text":"Users get custom links to recreate environments on CISL on-premise or 2i2c JupyterHub. Provide a public or private Container Registry to host images created from GitHub repository.","title":"BinderHub"},{"location":"overview/services/#web-hosting","text":"","title":"Web Hosting"},{"location":"overview/services/#visualization","text":"","title":"Visualization"},{"location":"overview/services/#documentation","text":"Offer dynamic or static web hosting. Provide unique publicly resolvable DNS names as defined by the user.","title":"Documentation"},{"location":"overview/services/#tutorials","text":"Control student access to available resources. Provide students with identical base image to level set starting point. Provision and tear down student workspaces in a quick and user friendly fashion. Allow console sharing & viewing to assist instructor troubleshooting.","title":"Tutorials"},{"location":"overview/services/#data-access","text":"","title":"Data Access"},{"location":"overview/services/#glade-read-only","text":"A read only NFS mount will be provided for us to utilize in providing access to datasets host on GLADE.","title":"GLADE Read Only"},{"location":"overview/services/#stratus-s3-buckets","text":"We will be Administrators for allocated space on STRAUS and will manage buckets and users. API access to buckets in the same fashion as AWS provide a familiar interface. Buckets can be used to host terraform state files if we decide to go that route.","title":"STRATUS S3 buckets"},{"location":"overview/services/#api","text":"We can utilize STRATUS and other offerings to create APIs to interact programmatically with datasets.","title":"API"},{"location":"overview/services/#file-sharing","text":"Offer a central location where model output and other data that has historically been hard to transfer can be served more easily to the users preferred location.","title":"File Sharing"},{"location":"overview/services/#loggingmetrics","text":"Provide interactive dashboards for compute, storage, and networking resources. Log all user interactions with our services in order to provide utilization and usage numbers. Make a determination for on-premise service feasibility based on the information collected. Performance on the teams ability to meet Service Level Agreements (SLAs) will be recorded.","title":"Logging/Metrics"},{"location":"overview/services/#documentation_1","text":"Overall vision and goals will be presented as the project evolves to provide full visibility in to the objectives. Implementation of all systems will be documented with code so that someone unfamiliar with the project could recreate the environment. Interactions with cloud offerings will be documented fully so that end users know what services are offered and how to obtain or use those services. Service Level Agreements (SLAs) will be provided so that users know what level of service they can expect with their implementation. Provide details on Agile implementation and how to work with the team via that implementation.","title":"Documentation"},{"location":"overview/services/#service-level-agreements-slas","text":"Defines and documents the operating procedures and commitments between providers and users. SLAs will be defined and documented for each service so users and maintainers can operate with the same expectations.","title":"Service Level Agreements (SLAs)"},{"location":"overview/services/#backupsdisaster-recovery-dr","text":"Each supported service will have an associated and documented Backup and DR policy as part of the SLAs.","title":"Backups/Disaster Recovery (DR)"},{"location":"overview/services/#request-for-enhancement-rfes","text":"Users will have an avenue for submitting RFEs and that process will be documented.","title":"Request For Enhancement (RFEs)"},{"location":"overview/use-cases/","text":"Use Cases Running Tutorials An on-premise cloud would allow hosting of user tutorials in an environment controlled by the organization. Access to systems would be controlled by administrators and tutorial leaders in a programmatic way speeding up the on-boarding and off-boarding process. Golden images that allow an identical starting point for each tutorial would be hosted on shared storage. These images would be deployed and assigned to users in a programmatic way by administrators and tutorial leaders. Remote viewing access for users and administrators simultaneously would also create an environment where it was easier to help troubleshoot issues during the tutorials. If possible this would be best done on a Web UI as trying to use SSH and VNC may have firewall complications that a Web UI can alleviate. Jupyter Services JupyterHub is running on a bare metal kubernetes (k8s) cluster deployed with Rancher RKE2. This instance will have authentication for individual users. A user will be able to select from a few different sized environments, some offering GPU capabilities, and a new instance will be spun up with KubeSpawner from a base image. The base image will contain a few common kernels and packages but users will be able to customize their instance with the packages they require. Shared storage will be provided with a unique directory for each user to keep their files. GLADE read-only access will also be provided to each instance so datasets can be accessed from an internal nearby location. Dask services will also be provided in the form of dask.distributed, dask-kubernetes, dask-gateway, and dask-jobqueue for running against HPC resources. Web based visualization capabilities will also be provided but access to these websites hosted via JupyterHub will be limited to authorized users of the JupyterHub instance. Dask on JupyterHub Dask dashboard can be used to view resource utilization and is automatically started via Bokeh once a Dask scheduler is created. Dask Arrays can be used in place of NumPy for parallelized processing of larger-than-memory Arrays. Dask DataFrames can be used in place of pandas for parallelized processing of larger-than-memory DataFrames. Utilize Xarray open_mfdataset & parallel=True to open multiple files as a single Xarray dataset in parallel with Dask Delayed. Dask Distributed can be utilized to create a local Dask Cluster or utilize HPC resources via dask-jobqueue. Running Project Pythia cookbooks such as this one . This notebook utilizes Xarray, Dask Array and Distributed together to allow calculation of the Meridional Overturning Circulation for a series of high resolution projections of ocean state under a climate change scenarios. GPUs on JupyterHub GPU enabled Jupyter notebook servers can be provided to users who want to take advantage of parallel processing on GPU cores. Dask distributed will also be able to create workers with GPUs in order to utilize python modules like RAPIDS and CuPy. Data Access GLADE Access GLADE Access will be provided to JupyterHub instances for users to read GLADE datasets in a close location. An NFS mount to other resources hosted on CISL on-prem cloud hardware may be made available as well. Object Storage CISL provides an Object Storage system named STRATUS with an API available to access S3 buckets in the same fashion as AWS. STRATUS will be utilized to provide API access to required datasets as well as possibly hosting terraform state files used in creating other user resources. Administrators on our STRATUS implementation will be responsible for user management and the creation and maintenance of new buckets. An elastic and flexible catalog for datasets will be implemented with preview or thumbnail methods to ensure access to the correct datasets. Uncoupling large datasets in to individual components will be part of the implementation process. Shared Storage Shared Storage will be provided to JupyterHub instances via the NFS protocol. Access to these mounts from other resources hosted on CISL on-prem cloud hardware may be made available as well. Local Transfers A method to allow for quick and easy data & output transfers to local systems will be provided with detailed instructions. Web Visualization Hosting Real time interactive and customizable web visualizations will be hosted in various methods on CISL infrastructure. Applications such as Bokeh, Panel, Viola, etc. can host interactive visualizations from Jupyter Notebooks or Python applications. Users will be provided with a programmatic way to deploy these applications with public or private URLs in a stable highly available environment. File Sharing The CISL on-prem infrastructure provides a location that is easier to access and share information and files. Applications like NextCloud can be provisioned to host file sharing services. Creating k8s clusters Rancher is part of the on-prem software stack and can be utilized to provision users their own k8s cluster for development or production use cases. Access to most storage resources can also be provided in these deployments. Hosting Containerized Applications Users will be able to deploy their containerized applications in to the k8s cluster which provides a highly available and accessible hosting location. Smaller individual JupyterLab containers can be provisioned with the ability to host public facing web visualizations via notebooks and code running in the JupyterLab containers. Hosting Virtual Machines (VMs) If needed, VM's can also be provided either from pre configured templates or as bare instances and will be hosted either in a VMware cluster or the k8s cluster. Continuous Integration (CI) Platform CI tools like Jenkins can be hosted to run automated test suites on new code deployments. Development Environment Development and testing the migration of existing workflows to platform independent micro services in a low/no cost environment.","title":"Use Cases"},{"location":"overview/use-cases/#use-cases","text":"","title":"Use Cases"},{"location":"overview/use-cases/#running-tutorials","text":"An on-premise cloud would allow hosting of user tutorials in an environment controlled by the organization. Access to systems would be controlled by administrators and tutorial leaders in a programmatic way speeding up the on-boarding and off-boarding process. Golden images that allow an identical starting point for each tutorial would be hosted on shared storage. These images would be deployed and assigned to users in a programmatic way by administrators and tutorial leaders. Remote viewing access for users and administrators simultaneously would also create an environment where it was easier to help troubleshoot issues during the tutorials. If possible this would be best done on a Web UI as trying to use SSH and VNC may have firewall complications that a Web UI can alleviate.","title":"Running Tutorials"},{"location":"overview/use-cases/#jupyter-services","text":"JupyterHub is running on a bare metal kubernetes (k8s) cluster deployed with Rancher RKE2. This instance will have authentication for individual users. A user will be able to select from a few different sized environments, some offering GPU capabilities, and a new instance will be spun up with KubeSpawner from a base image. The base image will contain a few common kernels and packages but users will be able to customize their instance with the packages they require. Shared storage will be provided with a unique directory for each user to keep their files. GLADE read-only access will also be provided to each instance so datasets can be accessed from an internal nearby location. Dask services will also be provided in the form of dask.distributed, dask-kubernetes, dask-gateway, and dask-jobqueue for running against HPC resources. Web based visualization capabilities will also be provided but access to these websites hosted via JupyterHub will be limited to authorized users of the JupyterHub instance.","title":"Jupyter Services"},{"location":"overview/use-cases/#dask-on-jupyterhub","text":"Dask dashboard can be used to view resource utilization and is automatically started via Bokeh once a Dask scheduler is created. Dask Arrays can be used in place of NumPy for parallelized processing of larger-than-memory Arrays. Dask DataFrames can be used in place of pandas for parallelized processing of larger-than-memory DataFrames. Utilize Xarray open_mfdataset & parallel=True to open multiple files as a single Xarray dataset in parallel with Dask Delayed. Dask Distributed can be utilized to create a local Dask Cluster or utilize HPC resources via dask-jobqueue. Running Project Pythia cookbooks such as this one . This notebook utilizes Xarray, Dask Array and Distributed together to allow calculation of the Meridional Overturning Circulation for a series of high resolution projections of ocean state under a climate change scenarios.","title":"Dask on JupyterHub"},{"location":"overview/use-cases/#gpus-on-jupyterhub","text":"GPU enabled Jupyter notebook servers can be provided to users who want to take advantage of parallel processing on GPU cores. Dask distributed will also be able to create workers with GPUs in order to utilize python modules like RAPIDS and CuPy.","title":"GPUs on JupyterHub"},{"location":"overview/use-cases/#data-access","text":"","title":"Data Access"},{"location":"overview/use-cases/#glade-access","text":"GLADE Access will be provided to JupyterHub instances for users to read GLADE datasets in a close location. An NFS mount to other resources hosted on CISL on-prem cloud hardware may be made available as well.","title":"GLADE Access"},{"location":"overview/use-cases/#object-storage","text":"CISL provides an Object Storage system named STRATUS with an API available to access S3 buckets in the same fashion as AWS. STRATUS will be utilized to provide API access to required datasets as well as possibly hosting terraform state files used in creating other user resources. Administrators on our STRATUS implementation will be responsible for user management and the creation and maintenance of new buckets. An elastic and flexible catalog for datasets will be implemented with preview or thumbnail methods to ensure access to the correct datasets. Uncoupling large datasets in to individual components will be part of the implementation process.","title":"Object Storage"},{"location":"overview/use-cases/#shared-storage","text":"Shared Storage will be provided to JupyterHub instances via the NFS protocol. Access to these mounts from other resources hosted on CISL on-prem cloud hardware may be made available as well.","title":"Shared Storage"},{"location":"overview/use-cases/#local-transfers","text":"A method to allow for quick and easy data & output transfers to local systems will be provided with detailed instructions.","title":"Local Transfers"},{"location":"overview/use-cases/#web-visualization-hosting","text":"Real time interactive and customizable web visualizations will be hosted in various methods on CISL infrastructure. Applications such as Bokeh, Panel, Viola, etc. can host interactive visualizations from Jupyter Notebooks or Python applications. Users will be provided with a programmatic way to deploy these applications with public or private URLs in a stable highly available environment.","title":"Web Visualization Hosting"},{"location":"overview/use-cases/#file-sharing","text":"The CISL on-prem infrastructure provides a location that is easier to access and share information and files. Applications like NextCloud can be provisioned to host file sharing services.","title":"File Sharing"},{"location":"overview/use-cases/#creating-k8s-clusters","text":"Rancher is part of the on-prem software stack and can be utilized to provision users their own k8s cluster for development or production use cases. Access to most storage resources can also be provided in these deployments.","title":"Creating k8s clusters"},{"location":"overview/use-cases/#hosting-containerized-applications","text":"Users will be able to deploy their containerized applications in to the k8s cluster which provides a highly available and accessible hosting location. Smaller individual JupyterLab containers can be provisioned with the ability to host public facing web visualizations via notebooks and code running in the JupyterLab containers.","title":"Hosting Containerized Applications"},{"location":"overview/use-cases/#hosting-virtual-machines-vms","text":"If needed, VM's can also be provided either from pre configured templates or as bare instances and will be hosted either in a VMware cluster or the k8s cluster.","title":"Hosting Virtual Machines (VMs)"},{"location":"overview/use-cases/#continuous-integration-ci-platform","text":"CI tools like Jenkins can be hosted to run automated test suites on new code deployments.","title":"Continuous Integration (CI) Platform"},{"location":"overview/use-cases/#development-environment","text":"Development and testing the migration of existing workflows to platform independent micro services in a low/no cost environment.","title":"Development Environment"},{"location":"slas/glade/","text":"GLADE SLA's GLADE is part of the Advanced Research Computing (ARC) group. Their documentation provides a lot of information about GLADE and the policies in place. /collections As part of the policy defined by ARC collections does not have a user quota and is not backed up. Data is also not purged from collections. /campaign As part of the policy defined by ARC campaign does not have a user quota and is not backed up. Data is also not purged from campaign. Dedicated project spaces on campaign are available through the allocations process GLADE Help Help with GLADE directly should be addressed by submitting a ticket to the NCAR Research Computing help desk . If there is an issue with accessing GLADE from the k8s JupyterHub please email cisl-on-prem-cloud@ucar.edu or open a ticket on the CCPP jira project","title":"GLADE"},{"location":"slas/glade/#glade-slas","text":"GLADE is part of the Advanced Research Computing (ARC) group. Their documentation provides a lot of information about GLADE and the policies in place.","title":"GLADE SLA's"},{"location":"slas/glade/#collections","text":"As part of the policy defined by ARC collections does not have a user quota and is not backed up. Data is also not purged from collections.","title":"/collections"},{"location":"slas/glade/#campaign","text":"As part of the policy defined by ARC campaign does not have a user quota and is not backed up. Data is also not purged from campaign. Dedicated project spaces on campaign are available through the allocations process","title":"/campaign"},{"location":"slas/glade/#glade-help","text":"Help with GLADE directly should be addressed by submitting a ticket to the NCAR Research Computing help desk . If there is an issue with accessing GLADE from the k8s JupyterHub please email cisl-on-prem-cloud@ucar.edu or open a ticket on the CCPP jira project","title":"GLADE Help"}]}